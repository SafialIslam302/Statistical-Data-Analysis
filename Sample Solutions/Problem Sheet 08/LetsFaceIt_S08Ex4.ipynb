{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd18067",
   "metadata": {},
   "source": [
    "**Solution for Sheet 08 - Exercise 4**\n",
    "\n",
    "Contributors: \n",
    "* Annika Bätz: annika.baetz@uni-potsdam.de\n",
    "* Max Serra: maximilia-manuel.serra.lasierra@uni-potsdam.de\n",
    "* Adrián de Miguel: adrian.de.miguel.palacio@uni-potsdam.de\n",
    "* Ignacio Llorca: ignacio.llorca.rodriguez@uni-potsdam.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd640f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cc65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(pd.read_csv('X.txt', header=None))\n",
    "y = np.array(pd.read_csv('Y.txt', header=None))\n",
    "\n",
    "# add column of 1's to allow bias\n",
    "X_extend = np.hstack((np.ones((len(X),1)),X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ffae9",
   "metadata": {},
   "source": [
    "Standard Regression without Regularization: $\\;\\;\\;$ $\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$\n",
    "\n",
    "Ridge Regression: $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ $\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X}+\\lambda I_p)^{-1}\\mathbf{X}^T\\mathbf{y}$\n",
    "\n",
    "$\\lambda$ is the regularization parameter. It is a hyperparameter that can be tuned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f427e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does X have full rank?\n",
    "X_extend.shape[1] == np.linalg.matrix_rank(X_extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce45b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardRegression(X, y):\n",
    "    return inv(X.T@X)@X.T@y\n",
    "\n",
    "def RidgeRegression(X, y, lbd):\n",
    "    return inv(X.T@X + lbd)@X.T@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab0f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(X, y, beta):\n",
    "    \"\"\" Computes the mean squared error \"\"\"\n",
    "    return np.mean((X@beta - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c48c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_bias(X, y, beta):\n",
    "    \"\"\" Computes the empirical bias: Mean residual \"\"\"\n",
    "    return np.mean(X@beta - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ba8c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emp_variance(X, y, beta):\n",
    "    \"\"\" Computes the empirical variance using the true values as the \"reference\" \"\"\"\n",
    "    return (1/(len(y)-1))*np.sum((X@beta - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bceb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_variance(X, y, beta):\n",
    "    \"\"\" Computes the sample variance using the mean predicted value as the \"reference\" \"\"\"\n",
    "    return (1/(len(y)-1))*np.sum((X@beta - np.mean(X@beta))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c0316d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta for Standard Regression:\n",
      "[[-0.00800698]\n",
      " [ 0.88161162]\n",
      " [-2.45938171]\n",
      " [-0.97715699]]\n",
      "\n",
      "Beta for Ridge Regression:\n",
      "[[ 0.00589151]\n",
      " [ 0.90744364]\n",
      " [-2.43410595]\n",
      " [-0.97683612]]\n"
     ]
    }
   ],
   "source": [
    "beta_stand = StandardRegression(X_extend, y)\n",
    "print('Beta for Standard Regression:')\n",
    "print(beta_stand)\n",
    "beta_ridge = RidgeRegression(X_extend, y, lbd=1)\n",
    "print('\\nBeta for Ridge Regression:')\n",
    "print(beta_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "327d4424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Regression:\n",
      "MSE: 0.9548405627555108\n",
      "Empirical Bias: -9.94229574291185e-17\n",
      "Empirical Variance: 0.9596147655692883\n",
      "Sample Variance: 10.959436916685817\n"
     ]
    }
   ],
   "source": [
    "print('Standard Regression:')\n",
    "mse_stand = mse(X_extend, y, beta_stand)\n",
    "print('MSE:', mse_stand)\n",
    "emp_bias_stand = emp_bias(X_extend, y, beta_stand)\n",
    "print('Empirical Bias:', emp_bias_stand)\n",
    "emp_variance_stand = emp_variance(X_extend, y, beta_stand)\n",
    "print('Empirical Variance:', emp_variance_stand)\n",
    "sample_variance_stand = sample_variance(X_extend, y, beta_stand)\n",
    "print('Sample Variance:', sample_variance_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90637144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression:\n",
      "MSE: 0.955652311528916\n",
      "Empirical Bias: 0.012425905038097639\n",
      "Empirical Variance: 0.9604305730865607\n",
      "Sample Variance: 10.89270682280516\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression:')\n",
    "mse_ridge = mse(X_extend, y, beta_ridge)\n",
    "print('MSE:', mse_ridge)\n",
    "emp_bias_ridge = emp_bias(X_extend, y, beta_ridge)\n",
    "print('Empirical Bias:', emp_bias_ridge)\n",
    "emp_variance_ridge = emp_variance(X_extend, y, beta_ridge)\n",
    "print('Empirical Variance:', emp_variance_ridge)\n",
    "sample_variance_ridge = sample_variance(X_extend, y, beta_ridge)\n",
    "print('Sample Variance:', sample_variance_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a7d81",
   "metadata": {},
   "source": [
    "The empirical variance is quite similar for the Standard Regression and the Ridge Regression. \n",
    "\n",
    "The empirical bias of the Standard Regression Model is very close to 0. That is expected since we saw in the lecture that the average residual for the Least Squares Estimator without Regularizer is 0. That the computed empircial bias is not exactly 0 is probably due to internal rounding operations when the computer has to handel long decimal numbers.  \n",
    "\n",
    "The empircial bias of the Ridge Regression Model is approximately 0.012. This means that on average we got slightly higher predicted values than the true value is. Let's try out different regularization parameters and see how these results change: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fbb849a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression with lambda=0.1:\n",
      "MSE: 0.9548406481876277\n",
      "Empirical Bias: 0.00012747581415444\n",
      "Empirical Variance: 0.959614851428566\n",
      "Sample Variance: 10.958745633135583\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression with lambda=0.1:')\n",
    "beta_ridge = RidgeRegression(X_extend, y, lbd=0.01)\n",
    "mse_ridge = mse(X_extend, y, beta_ridge)\n",
    "print('MSE:', mse_ridge)\n",
    "emp_bias_ridge = emp_bias(X_extend, y, beta_ridge)\n",
    "print('Empirical Bias:', emp_bias_ridge)\n",
    "emp_variance_ridge = emp_variance(X_extend, y, beta_ridge)\n",
    "print('Empirical Variance:', emp_variance_ridge)\n",
    "sample_variance_ridge = sample_variance(X_extend, y, beta_ridge)\n",
    "print('Sample Variance:', sample_variance_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "361888c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression with lambda=0.1:\n",
      "MSE: 0.9548490658972969\n",
      "Empirical Bias: 0.0012717651507996972\n",
      "Empirical Variance: 0.9596233112267833\n",
      "Sample Variance: 10.952546538291537\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression with lambda=0.1:')\n",
    "beta_ridge = RidgeRegression(X_extend, y, lbd=0.1)\n",
    "mse_ridge = mse(X_extend, y, beta_ridge)\n",
    "print('MSE:', mse_ridge)\n",
    "emp_bias_ridge = emp_bias(X_extend, y, beta_ridge)\n",
    "print('Empirical Bias:', emp_bias_ridge)\n",
    "emp_variance_ridge = emp_variance(X_extend, y, beta_ridge)\n",
    "print('Empirical Variance:', emp_variance_ridge)\n",
    "sample_variance_ridge = sample_variance(X_extend, y, beta_ridge)\n",
    "print('Sample Variance:', sample_variance_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d83b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression with lambda=1:\n",
      "MSE: 0.955652311528916\n",
      "Empirical Bias: 0.012425905038097639\n",
      "Empirical Variance: 0.9604305730865607\n",
      "Sample Variance: 10.89270682280516\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression with lambda=1:')\n",
    "beta_ridge = RidgeRegression(X_extend, y, lbd=1)\n",
    "mse_ridge = mse(X_extend, y, beta_ridge)\n",
    "print('MSE:', mse_ridge)\n",
    "emp_bias_ridge = emp_bias(X_extend, y, beta_ridge)\n",
    "print('Empirical Bias:', emp_bias_ridge)\n",
    "emp_variance_ridge = emp_variance(X_extend, y, beta_ridge)\n",
    "print('Empirical Variance:', emp_variance_ridge)\n",
    "sample_variance_ridge = sample_variance(X_extend, y, beta_ridge)\n",
    "print('Sample Variance:', sample_variance_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef54444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression with lambda=5:\n",
      "MSE: 0.9715527201013028\n",
      "Empirical Bias: 0.05638109000121817\n",
      "Empirical Variance: 0.9764104837018093\n",
      "Sample Variance: 10.667260379162668\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression with lambda=5:')\n",
    "beta_ridge = RidgeRegression(X_extend, y, lbd=5)\n",
    "mse_ridge = mse(X_extend, y, beta_ridge)\n",
    "print('MSE:', mse_ridge)\n",
    "emp_bias_ridge = emp_bias(X_extend, y, beta_ridge)\n",
    "print('Empirical Bias:', emp_bias_ridge)\n",
    "emp_variance_ridge = emp_variance(X_extend, y, beta_ridge)\n",
    "print('Empirical Variance:', emp_variance_ridge)\n",
    "sample_variance_ridge = sample_variance(X_extend, y, beta_ridge)\n",
    "print('Sample Variance:', sample_variance_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "359520a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression with lambda=10:\n",
      "MSE: 1.008547854671894\n",
      "Empirical Bias: 0.10107269293922573\n",
      "Empirical Variance: 1.0135905939452534\n",
      "Sample Variance: 10.454987741640672\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression with lambda=10:')\n",
    "beta_ridge = RidgeRegression(X_extend, y, lbd=10)\n",
    "mse_ridge = mse(X_extend, y, beta_ridge)\n",
    "print('MSE:', mse_ridge)\n",
    "emp_bias_ridge = emp_bias(X_extend, y, beta_ridge)\n",
    "print('Empirical Bias:', emp_bias_ridge)\n",
    "emp_variance_ridge = emp_variance(X_extend, y, beta_ridge)\n",
    "print('Empirical Variance:', emp_variance_ridge)\n",
    "sample_variance_ridge = sample_variance(X_extend, y, beta_ridge)\n",
    "print('Sample Variance:', sample_variance_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7588e",
   "metadata": {},
   "source": [
    "If we increase the regularization parameter, three of our metrics (MSE, emp. bias and emp. variance) increase, whereas the sample variance decreases slightly. Most interesting is the increase in the empirical bias. This shows that using a regularizer is a trade-off between a in general more stable solution (when using regularization; not shown here) and a small empirical bias/ small mean residual (less regularization).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63516283",
   "metadata": {},
   "source": [
    "## 1.Little Experiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fa9d1",
   "metadata": {},
   "source": [
    "If we use Regularization we are introducing a bias to our model. However, we can expect our model building to be more stable. This means, if we are slightly changing our training data, the trained models should still be quite similar for the regularized approach, whereas we can expect the models without regularization to differ more. The latter approach is in general also more prone to overfitting to the data. \n",
    "\n",
    "The goal is to find a good balance between bias and variance, i.e. to build a stable model that generalizes well, doesn't overfit, but still catches the main structure of the data. This is also known as the \"Bias-Variance-Tradeoff\". \n",
    "\n",
    "In order to show this here, we have constructed a little experiment: \n",
    "* First we split the data into two parts: \"inner\" and \"test\" data\n",
    "* Then we repeat for 10 iterations: \n",
    "    * Split the \"inner\" part again into \"train\" and \"remain\". The \"remain\" part is not needed anymore (for this iteration). We train a model on the \"train\" part and make a prediction on the \"test\" set. \n",
    "    * We compute the empirical bias for this model\n",
    "* Now we have several lists of predictions on the test data. For each true label in the test data, we can compute the variance of all the different predicted values: \n",
    "    * Let $n$ be the number of trained models\n",
    "    * Let $y_i$ be the true label for the i-th instance in the test data.\n",
    "    * Let $\\hat{y}_{i1}, \\hat{y}_{i2}, ..., \\hat{y}_{in}$ be the different predictions for $y_i$\n",
    "    * Let $\\bar{y}_i$ be the mean value of $\\hat{y}_{i1}, \\hat{y}_{i2}, ..., \\hat{y}_{in}$\n",
    "    * We define the variance for the predictions for one specific instance as: $var_i = \\frac{1}{n-1} \\sum_{k=1}^{n}(\\hat{y}_{ik} - \\bar{y}_i)^2$\n",
    "    * As a final step we can compute the mean value of $var_i$ for all instances $i$ in the test data. \n",
    "    \n",
    "The main difference to the first part of this notebook is that we now use different data to train and test our models and that we repeat the whole procedure several times to get a more general solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6e4b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3c1b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_estimate(X, y, rounds=10, model='simple', lbd=0.1):\n",
    "    X_inner, X_test, y_inner, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    pred_list = []\n",
    "    bias_list = []\n",
    "    for i in range(rounds):\n",
    "        # divide the data into training and test data\n",
    "        X_train, X_remain, y_train, y_remain = train_test_split(X_inner, y_inner, test_size=0.3, random_state=i)\n",
    "        \n",
    "        # compute beta:\n",
    "        if model=='simple':\n",
    "            beta = StandardRegression(X_train, y_train)\n",
    "        elif model=='ridge':\n",
    "            beta = RidgeRegression(X_train, y_train, lbd)\n",
    "            \n",
    "        # compute the empirical bias\n",
    "        bias_list.append(emp_bias(X_test, y_test, beta))\n",
    "            \n",
    "        # make prediction on the \"outer\" data\n",
    "        pred = X_test@beta\n",
    "        pred_list.append(pred)\n",
    "        \n",
    "    # compute the mean variance between all predictions\n",
    "    pred = np.array(pred_list)\n",
    "    n = rounds\n",
    "    variance = np.mean(1/(n-1) * np.sum((pred - np.mean(pred, axis=0))**2, axis=0))\n",
    "    return variance, bias_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c864c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Regression:\n",
      "Variance among the model predictions:  0.0097\n",
      "Mean empirical bias:  0.0082\n"
     ]
    }
   ],
   "source": [
    "print('Normal Regression:')\n",
    "variance_stand, bias_list_stand = bias_variance_estimate(X_extend, y, rounds=10, model='simple', lbd=0.1)\n",
    "print('Variance among the model predictions: ', np.round(variance_stand,4))\n",
    "print('Mean empirical bias: ', np.round(np.mean(bias_list_stand), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2af446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (lambda = 0.1):\n",
      "Variance among the model predictions:  0.0097\n",
      "Mean empirical bias:  0.0096\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression (lambda = 0.1):')\n",
    "variance, bias_list = bias_variance_estimate(X_extend, y, rounds=10, model='ridge', lbd=0.1)\n",
    "print('Variance among the model predictions: ', np.round(variance,4))\n",
    "print('Mean empirical bias: ', np.round(np.mean(bias_list), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04c1b2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (lambda = 1):\n",
      "Variance among the model predictions:  0.0094\n",
      "Mean empirical bias:  0.0216\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression (lambda = 1):')\n",
    "variance, bias_list = bias_variance_estimate(X_extend, y, rounds=10, model='ridge', lbd=1)\n",
    "print('Variance among the model predictions: ', np.round(variance,4))\n",
    "print('Mean empirical bias: ', np.round(np.mean(bias_list), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a033e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (lambda = 10):\n",
      "Variance among the model predictions:  0.0081\n",
      "Mean empirical bias:  0.1012\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression (lambda = 10):')\n",
    "variance, bias_list = bias_variance_estimate(X_extend, y, rounds=10, model='ridge', lbd=10)\n",
    "print('Variance among the model predictions: ', np.round(variance,4))\n",
    "print('Mean empirical bias: ', np.round(np.mean(bias_list), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9260994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression (lambda = 50):\n",
      "Variance among the model predictions:  0.0083\n",
      "Mean empirical bias:  0.2056\n"
     ]
    }
   ],
   "source": [
    "print('Ridge Regression (lambda = 50):')\n",
    "variance, bias_list = bias_variance_estimate(X_extend, y, rounds=10, model='ridge', lbd=50)\n",
    "print('Variance among the model predictions: ', np.round(variance,4))\n",
    "print('Mean empirical bias: ', np.round(np.mean(bias_list), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279a18c",
   "metadata": {},
   "source": [
    "The results fit our expectations quite well: \n",
    "\n",
    "By introducing regularization, we introduce a bias, the computed mean empirical bias increases. If we regularize stronger (increase the regularization parameter lambda), the bias increases even more. \n",
    "\n",
    "On the other hand, the variance among the different model predictions decreases when we use regularization. This means the model building is less effected by the difference in training data. \n",
    "\n",
    "In our specific case, we can see that the variance decreases only to a certain point. If the hyperparameter is quite high (lambda = 50) the variance starts to increase again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5351ea17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
